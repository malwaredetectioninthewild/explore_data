{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# The timestamp we used to split our training and testing samples \n",
    "# according to their first seen timestamps on VirusTotal\n",
    "SPLIT_TIMESTAMP = 1522540800 \n",
    "\n",
    "# whether to use the small sample datasets included in the repository\n",
    "# set this to False to use the full datasets that we will share upon requests\n",
    "use_sample_datasets = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<DatasetName>_sandbox_metadata.json` and ``<DatasetName>_endpoint_metadata.json`` files include the metadata for each hash (sample) collected for our datasets. Let's load them from the json files included in this repository (after extracting the `.zip` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files are included in the Train Data\n",
    "\n",
    "if use_sample_datasets:\n",
    "    train_data_path = 'MalwareITW_TrainData_Sample' # small train metadata, included in the repository\n",
    "else:\n",
    "    train_data_path = 'MalwareITW_TrainData' # full train metadata, shared when requested\n",
    "\n",
    "with open(os.path.join(train_data_path, 'train_sandbox_metadata.json'), 'r') as fp:\n",
    "    train_sandbox_metadata = json.load(fp)\n",
    "\n",
    "with open(os.path.join(train_data_path, 'train_endpoint_metadata.json'), 'r') as fp:\n",
    "    train_endpoint_metadata = json.load(fp)\n",
    "\n",
    "# These files are included in the TrainAndTest Data\n",
    "\n",
    "if use_sample_datasets:\n",
    "    full_data_path = 'MalwareITW_TrainAndTestData_Sample' # small TrainAndTest metadata, included in the repository\n",
    "else:\n",
    "    full_data_path = 'MalwareITW_TrainAndTestData' # full TrainAndTest metadata, shared when requested\n",
    "\n",
    "with open(os.path.join(full_data_path, 'TrainAndTest_sandbox_metadata.json'), 'r') as fp:\n",
    "    full_sandbox_metadata = json.load(fp)\n",
    "\n",
    "with open(os.path.join(full_data_path, 'TrainAndTest_endpoint_metadata.json'), 'r') as fp:\n",
    "    full_endpoint_metadata = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the metadata in the **Train Data** for the `Habo` sandbox sample with hash: `9deb78f23cb5a7876992af03ca43acf5b9d9f000c22d62310df265e6ad4945ba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1518344430,\n",
       " 'publisher': 'Paul Mattes',\n",
       " 'vhash': '0160866d1c0d1c051505105016z1c9z5bz1fz',\n",
       " 'tlsh': 'T1E0853353E0B240BAE2B2D93D1C3A96245A237D6279B596183F8C9EDE1F33743190F356',\n",
       " 'ssdeep': '24576:nnaVefuUQs5z3+x36czfY76eoPVCEX6BqR7YkfU3ZLvkdt9vkxlgMZUlYWh7Z0xx:na0iMY6d7E6BqRYZLMwgMZUea7s+HQN5',\n",
       " 'dataset_name': ['sorel18'],\n",
       " 'old': {'label': 0,\n",
       "  'fam': 'BENIGN',\n",
       "  'scan_date': 1518344430,\n",
       "  'num_detections': -1,\n",
       "  'label_source': 'sorel18'},\n",
       " 'file_names': ['wc3270-3.6ga5-setup.exe',\n",
       "  'managedway.dl.sourceforge.net',\n",
       "  'downloads.sourceforge.net',\n",
       "  'astuteinternet.dl.sourceforge.net',\n",
       "  '9DEB78F23CB5A7876992AF03CA43ACF5B9D9F000C22D62310DF265E6AD4945BA.exe']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sandbox_metadata['9deb78f23cb5a7876992af03ca43acf5b9d9f000c22d62310df265e6ad4945ba']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `first_seen`: Epoch timestamp that corresponds to the first seen date of the sample on VirusTotal\n",
    "* `publisher`: The publisher information for the sample, extracted from the VirusTotal report. \n",
    "* `vhash`: The vhash hash of the binary file, included in its VirusTotal report.\n",
    " * `tlsh`: The tlsh hash of the binary file, included in its VirusTotal report.\n",
    " * `ssdeep`: The ssdeep hash of the binary file, included in its VirusTotal report.\n",
    "* `dataset_name`: The source dataset of this sample. In our work, we merge multiple datasets, including *SOREL*, *EMBER* and VirusTotal Malware Folder (*vt17* and *vt18*). The same sample can be seen in multiple sources. The dataset name *ep* corresponds to the samples in our endpoint dataset.\n",
    "* `file_names`: This is the list of file names this sample had in its submissions to VirusTotal.\n",
    "\n",
    "There is also a sub-dictionary in the Train Data metadata with the key `old`:\n",
    "\n",
    "This sub-dictionary contains the label information from a VirusTotal report that is older than the sample's `first_seen` timestamp. These older reports are not available for all samples, in which case, we used the most recent report to populate this sub-dictionary.\n",
    "\n",
    "The keys in this sub-dictionary:\n",
    "\n",
    "* `label`: This is the ground truth label we assigned to this sample (e.g., if number of detections is over 5, the sample is labeled as malware). Label 0,1 and 2 are for benign, malware and PUP samples, respectively.\n",
    "* `family`: This is the malware family tag of the sample, assigned using AVClass2. This tag is *BENIGN* if the sample is benign or *UNKNOWN* if the family tag is unavailable.\n",
    "* `scan_date`: The timestamp of the VirusTotal report used to label this sample.\n",
    "* `num_detection`: This is the number of AV engines on VirusTotal that detected this sample as malware, if it is -1, our labeling source was not VirusTotal.\n",
    "* `label_source`: This indicates where our older VirusTotal report came from. For example, *latest_copied* means that we didn't have an older detection report for this sample and used the latest report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the metadata of a sample in our endpoint dataset with hash `b6a5ca3c796ddc03b63d233a89095dbc655bc1fe4d1a1e9a520901656ece918b`. \n",
    "\n",
    "This record has the same structure as sandbox metadata records. All samples in this metadata file will include *ep* in the `dataset_name` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1269478350,\n",
       " 'publisher': 'NO SIGNATURE',\n",
       " 'vhash': '044046151d155dzd=z',\n",
       " 'tlsh': 'T15803728B36E7C666ED890B755E9AD6886517BC02DD10460B3ABC3F8FD9B52C24C44EC3',\n",
       " 'ssdeep': '384:wnM4A2o6PJhDex6wl6sSeTO3/JF1a/wiS40RAC:wPThxexd3K3UYiRkAC',\n",
       " 'dataset_name': ['ep'],\n",
       " 'old': {'fam': 'BENIGN',\n",
       "  'label': 0,\n",
       "  'scan_date': 1654318924,\n",
       "  'num_detections': 1,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'file_names': ['perl.exe',\n",
       "  '0a65acffd253a575c2aae13c523a7d99',\n",
       "  'perl5.8.3.exe']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_endpoint_metadata['b6a5ca3c796ddc03b63d233a89095dbc655bc1fe4d1a1e9a520901656ece918b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to collect the binary labels and families for all our endpoint hashes, you can do as follows:\n",
    "\n",
    "P.S. The small endpoint metadata included in this repository only contains three samples, request access for the full metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({1: 1, 2: 1, 0: 1})\n",
      "\n",
      "Family Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({'alien': 1, 'loadmoney': 1, 'BENIGN': 1})\n",
      "\n",
      "Publisher Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({'NO SIGNATURE': 2, 'OOO YULIYA': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "endpoint_train_hashes = [h for h, v in train_endpoint_metadata.items()]\n",
    "labels = []\n",
    "families = []\n",
    "publishers = []\n",
    "\n",
    "for h in endpoint_train_hashes:\n",
    "    metadata = train_endpoint_metadata[h]\n",
    "    labels.append(metadata['old']['label'])\n",
    "    families.append(metadata['old']['fam'])\n",
    "    publishers.append(metadata['publisher'])\n",
    "\n",
    "print('Label Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(labels))\n",
    "\n",
    "print('\\nFamily Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(families))\n",
    "\n",
    "print('\\nPublisher Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(publishers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we looked at the records included in the **Train Data**. \n",
    "\n",
    "Now, let's look at a record included in the **Full Data** for the same endpoint sample with hash `b6a5ca3c796ddc03b63d233a89095dbc655bc1fe4d1a1e9a520901656ece918b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1269478350,\n",
       " 'publisher': 'NO SIGNATURE',\n",
       " 'vhash': '044046151d155dzd=z',\n",
       " 'tlsh': 'T15803728B36E7C666ED890B755E9AD6886517BC02DD10460B3ABC3F8FD9B52C24C44EC3',\n",
       " 'ssdeep': '384:wnM4A2o6PJhDex6wl6sSeTO3/JF1a/wiS40RAC:wPThxexd3K3UYiRkAC',\n",
       " 'dataset_name': ['ep'],\n",
       " 'new': {'fam': 'BENIGN',\n",
       "  'label': 0,\n",
       "  'scan_date': 1654318924,\n",
       "  'num_detections': 1,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'old': {'fam': 'BENIGN',\n",
       "  'label': 0,\n",
       "  'scan_date': 1654318924,\n",
       "  'num_detections': 1,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'file_names': ['perl.exe',\n",
       "  '0a65acffd253a575c2aae13c523a7d99',\n",
       "  'perl5.8.3.exe']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_endpoint_metadata['b6a5ca3c796ddc03b63d233a89095dbc655bc1fe4d1a1e9a520901656ece918b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the **Full Data** metadata records include an additional sub-dictionary with the key `new`.\n",
    "\n",
    "This sub-dictionary contains the label information from the most recent VirusTotal report (collected in 2022).\n",
    "The structure of the `new` sub-dictionary is the same as the `old` sub-dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Full Data** metadata files also includes samples that are seen after the `SPLIT_TIMESTAMP`. These samples correspond to the testing samples in our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #samples in the train_sandbox_metadata: 30\n",
      "Total #samples in the train_endpoint_metadata: 3\n",
      "Total #samples in the train_sandbox_metadata seen after <SPLIT_TIMESTAMP>: 0\n",
      "Total #samples in the train_endpoint_metadata seen after <SPLIT_TIMESTAMP>: 0\n",
      "\n",
      "==================\n",
      "\n",
      "Total #samples in the full_sandbox_metadata: 60\n",
      "Total #samples in the full_endpoint_metadata: 6\n",
      "Total #samples in the full_sandbox_metadata seen after <SPLIT_TIMESTAMP>: 30\n",
      "Total #samples in the full_endpoint_metadata seen after <SPLIT_TIMESTAMP>: 3\n"
     ]
    }
   ],
   "source": [
    "# the metadata in the Train Data doesn't include any sample first seen after <SPLIT_TIMESTAMP>\n",
    "\n",
    "print(f'Total #samples in the train_sandbox_metadata: {len(train_sandbox_metadata)}')\n",
    "print(f'Total #samples in the train_endpoint_metadata: {len(train_endpoint_metadata)}')\n",
    "\n",
    "sb_test_samples_in_train = [h for h in train_sandbox_metadata if train_sandbox_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "ep_test_samples_in_train = [h for h in train_endpoint_metadata if train_endpoint_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "\n",
    "print(f'Total #samples in the train_sandbox_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_train)}')\n",
    "print(f'Total #samples in the train_endpoint_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_train)}')\n",
    "\n",
    "\n",
    "print('\\n==================\\n')\n",
    "\n",
    "# the metadata in the Full Data also includes samples first seen after <SPLIT_TIMESTAMP> (testing samples)\n",
    "\n",
    "print(f'Total #samples in the full_sandbox_metadata: {len(full_sandbox_metadata)}')\n",
    "print(f'Total #samples in the full_endpoint_metadata: {len(full_endpoint_metadata)}')\n",
    "\n",
    "sb_test_samples_in_full = [h for h in full_sandbox_metadata if full_sandbox_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "ep_test_samples_in_full = [h for h in full_endpoint_metadata if full_endpoint_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "\n",
    "print(f'Total #samples in the full_sandbox_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_full)}')\n",
    "print(f'Total #samples in the full_endpoint_metadata seen after <SPLIT_TIMESTAMP>: {len(ep_test_samples_in_full)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
