{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# The timestamp we used to split our training and testing samples \n",
    "# according to their first seen timestamps on VirusTotal\n",
    "SPLIT_TIMESTAMP = 1522540800 \n",
    "\n",
    "# whether to use the small sample datasets included in the repository\n",
    "# set this to False to use the full datasets that we will share upon requests\n",
    "use_sample_datasets = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<DatasetName>_sandbox_metadata.json` and ``<DatasetName>_endpoint_metadata.json`` files include the metadata for each hash (sample) collected for our datasets. Let's load them from the json files included in this repository (after extracting the `.zip` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files are included in the Train Data\n",
    "\n",
    "if use_sample_datasets:\n",
    "    train_data_path = 'MalwareITW_TrainData_Sample' # small train metadata, included in the repository\n",
    "else:\n",
    "    train_data_path = 'MalwareITW_TrainData' # full train metadata, shared when requested\n",
    "\n",
    "with open(os.path.join(train_data_path, 'Train_sandbox_metadata.json'), 'r') as fp:\n",
    "    train_sandbox_metadata = json.load(fp)\n",
    "\n",
    "with open(os.path.join(train_data_path, 'Train_endpoint_metadata.json'), 'r') as fp:\n",
    "    train_endpoint_metadata = json.load(fp)\n",
    "\n",
    "# These files are included in the TrainAndTest Data\n",
    "\n",
    "if use_sample_datasets:\n",
    "    traintest_data_path = 'MalwareITW_TrainAndTestData_Sample' # small TrainAndTest metadata, included in the repository\n",
    "else:\n",
    "    traintest_data_path = 'MalwareITW_TrainAndTestData' # full TrainAndTest metadata, shared when requested\n",
    "\n",
    "with open(os.path.join(traintest_data_path, 'TrainAndTest_sandbox_metadata.json'), 'r') as fp:\n",
    "    traintest_sandbox_metadata = json.load(fp)\n",
    "\n",
    "with open(os.path.join(traintest_data_path, 'TrainAndTest_endpoint_metadata.json'), 'r') as fp:\n",
    "    traintest_endpoint_metadata = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the metadata in the **Train Data** for the `Habo` sandbox sample with hash: `b3ac33c3156668b71d77f041ea48ad97fe96959f5ea1bc7e7e0695b29ab623d4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1519065442,\n",
       " 'publisher': 'Cloud Installer',\n",
       " 'vhash': '016056655d1555616ze006f7zf0c5z30400411z6bz',\n",
       " 'tlsh': 'T1DF3517316AC18031D3123331CE14EEEE356A6DB40DDA955FE2A43B394BB41B2DD3B65A',\n",
       " 'ssdeep': '12288:vsM+aTA3c+FK1vrlVYBVignBtZnfVq4cz1i5pP9kPQK:UV4W8hqBYgnBLfVqx1Wjk3',\n",
       " 'dataset_name': ['ember'],\n",
       " 'old': {'label': 2,\n",
       "  'scan_date': 1519065442,\n",
       "  'num_detections': -1,\n",
       "  'label_source': 'ember',\n",
       "  'fam': 'GENERIC_MAL'},\n",
       " 'file_names': ['IESettings',\n",
       "  'myfile.exe',\n",
       "  'VirusShare_e15a3a7731e9fb379ec15fce60466399']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sandbox_metadata['b3ac33c3156668b71d77f041ea48ad97fe96959f5ea1bc7e7e0695b29ab623d4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `first_seen`: Epoch timestamp that corresponds to the first seen date of the sample on VirusTotal\n",
    "* `publisher`: The publisher information for the sample, extracted from the VirusTotal report. \n",
    "* `vhash`: The vhash hash of the binary file, included in its VirusTotal report.\n",
    " * `tlsh`: The tlsh hash of the binary file, included in its VirusTotal report.\n",
    " * `ssdeep`: The ssdeep hash of the binary file, included in its VirusTotal report.\n",
    "* `dataset_name`: The source dataset of this sample. In our work, we merge multiple datasets, including *SOREL*, *EMBER* and VirusTotal Malware Folder (*vt17* and *vt18*). The same sample can be seen in multiple sources. The dataset name *ep* corresponds to the samples in our endpoint dataset.\n",
    "* `file_names`: This is the list of file names this sample had in its submissions to VirusTotal.\n",
    "\n",
    "There is also a sub-dictionary in the Train Data metadata with the key `old`:\n",
    "\n",
    "This sub-dictionary contains the label information from a VirusTotal report that is older than the sample's `first_seen` timestamp. These older reports are not available for all samples, in which case, we used the most recent report to populate this sub-dictionary.\n",
    "\n",
    "The keys in this sub-dictionary:\n",
    "\n",
    "* `label`: This is the ground truth label we assigned to this sample (e.g., if number of detections is over 5, the sample is labeled as malware). Label 0,1 and 2 are for benign, malware and PUP samples, respectively.\n",
    "* `family`: This is the malware family tag of the sample, assigned using AVClass2. This tag is *BENIGN* if the sample is benign or *UNKNOWN* if the family tag is unavailable.\n",
    "* `scan_date`: The timestamp of the VirusTotal report used to label this sample.\n",
    "* `num_detection`: This is the number of AV engines on VirusTotal that detected this sample as malware, if it is -1, our labeling source was not VirusTotal.\n",
    "* `label_source`: This indicates where our older VirusTotal report came from. For example, *latest_copied* means that we didn't have an older detection report for this sample and used the latest report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the metadata of a sample in our endpoint dataset with hash `f51a39a1735e1f1dd9d5c7cea3bd56a8dc4ba6f0b03747455778f73f0d78409a`. \n",
    "\n",
    "This record has the same structure as sandbox metadata records. All samples in this metadata file will include *ep* in the `dataset_name` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1521386004,\n",
       " 'publisher': 'TOV Dveri Fado',\n",
       " 'vhash': '095046651d751az3dnz95z17z',\n",
       " 'tlsh': 'none',\n",
       " 'ssdeep': '12288:bv2Jtp8DMW4chwIM+5he99cJFwOijTJCqbtFW1RY6NoEL/UD:GpHW5g9cJFIN6Rt5DUD',\n",
       " 'dataset_name': ['ep'],\n",
       " 'old': {'fam': 'cnopa',\n",
       "  'label': 1,\n",
       "  'scan_date': 1600864279,\n",
       "  'num_detections': 59,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'file_names': ['8e1d.tmp', 'ComDev.exe', 'nvfontcache.exe']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_endpoint_metadata['f51a39a1735e1f1dd9d5c7cea3bd56a8dc4ba6f0b03747455778f73f0d78409a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to collect the binary labels and families for all our endpoint hashes, you can do as follows:\n",
    "\n",
    "P.S. The small endpoint metadata included in this repository only contains three samples, request access for the full metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({2: 1, 0: 1, 1: 1})\n",
      "\n",
      "Family Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({'installcore': 1, 'BENIGN': 1, 'cnopa': 1})\n",
      "\n",
      "Publisher Counts in our Endpoint Samples Based on Train Data metadata:\n",
      "Counter({'NO SIGNATURE': 2, 'TOV Dveri Fado': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "endpoint_train_hashes = [h for h, v in train_endpoint_metadata.items()]\n",
    "labels = []\n",
    "families = []\n",
    "publishers = []\n",
    "\n",
    "for h in endpoint_train_hashes:\n",
    "    metadata = train_endpoint_metadata[h]\n",
    "    labels.append(metadata['old']['label'])\n",
    "    families.append(metadata['old']['fam'])\n",
    "    publishers.append(metadata['publisher'])\n",
    "\n",
    "print('Label Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(labels))\n",
    "\n",
    "print('\\nFamily Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(families))\n",
    "\n",
    "print('\\nPublisher Counts in our Endpoint Samples Based on Train Data metadata:')\n",
    "print(Counter(publishers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we looked at the records included in the **Train Data**. \n",
    "\n",
    "Now, let's look at a record included in the **TrainAndTest Data** for the same endpoint sample with hash `f51a39a1735e1f1dd9d5c7cea3bd56a8dc4ba6f0b03747455778f73f0d78409a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_seen': 1521386004,\n",
       " 'publisher': 'TOV Dveri Fado',\n",
       " 'vhash': '095046651d751az3dnz95z17z',\n",
       " 'tlsh': 'none',\n",
       " 'ssdeep': '12288:bv2Jtp8DMW4chwIM+5he99cJFwOijTJCqbtFW1RY6NoEL/UD:GpHW5g9cJFIN6Rt5DUD',\n",
       " 'dataset_name': ['ep'],\n",
       " 'new': {'fam': 'cnopa',\n",
       "  'label': 1,\n",
       "  'scan_date': 1600864279,\n",
       "  'num_detections': 59,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'old': {'fam': 'cnopa',\n",
       "  'label': 1,\n",
       "  'scan_date': 1600864279,\n",
       "  'num_detections': 59,\n",
       "  'label_source': 'latest_copied'},\n",
       " 'file_names': ['8e1d.tmp', 'ComDev.exe', 'nvfontcache.exe']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest_endpoint_metadata['f51a39a1735e1f1dd9d5c7cea3bd56a8dc4ba6f0b03747455778f73f0d78409a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the **TrainAndTest Data** metadata records include an additional sub-dictionary with the key `new`.\n",
    "\n",
    "This sub-dictionary contains the label information from the most recent VirusTotal report (collected in 2022).\n",
    "The structure of the `new` sub-dictionary is the same as the `old` sub-dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **TrainAndTest Data** metadata files also includes samples that are seen after the `SPLIT_TIMESTAMP`. These samples correspond to the testing samples in our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #samples in the train_sandbox_metadata: 30\n",
      "Total #samples in the train_endpoint_metadata: 3\n",
      "Total #samples in the train_sandbox_metadata seen after <SPLIT_TIMESTAMP>: 0\n",
      "Total #samples in the train_endpoint_metadata seen after <SPLIT_TIMESTAMP>: 0\n",
      "\n",
      "==================\n",
      "\n",
      "Total #samples in the traintest_sandbox_metadata: 60\n",
      "Total #samples in the traintest_endpoint_metadata: 6\n",
      "Total #samples in the traintest_sandbox_metadata seen after <SPLIT_TIMESTAMP>: 30\n",
      "Total #samples in the traintest_endpoint_metadata seen after <SPLIT_TIMESTAMP>: 3\n"
     ]
    }
   ],
   "source": [
    "# the metadata in the Train Data doesn't include any sample first seen after <SPLIT_TIMESTAMP>\n",
    "\n",
    "print(f'Total #samples in the train_sandbox_metadata: {len(train_sandbox_metadata)}')\n",
    "print(f'Total #samples in the train_endpoint_metadata: {len(train_endpoint_metadata)}')\n",
    "\n",
    "sb_test_samples_in_train = [h for h in train_sandbox_metadata if train_sandbox_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "ep_test_samples_in_train = [h for h in train_endpoint_metadata if train_endpoint_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "\n",
    "print(f'Total #samples in the train_sandbox_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_train)}')\n",
    "print(f'Total #samples in the train_endpoint_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_train)}')\n",
    "\n",
    "\n",
    "print('\\n==================\\n')\n",
    "\n",
    "# the metadata in the TrainAndTest Data also includes samples first seen after <SPLIT_TIMESTAMP> (testing samples)\n",
    "\n",
    "print(f'Total #samples in the traintest_sandbox_metadata: {len(traintest_sandbox_metadata)}')\n",
    "print(f'Total #samples in the traintest_endpoint_metadata: {len(traintest_endpoint_metadata)}')\n",
    "\n",
    "sb_test_samples_in_full = [h for h in traintest_sandbox_metadata if traintest_sandbox_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "ep_test_samples_in_full = [h for h in traintest_endpoint_metadata if traintest_endpoint_metadata[h]['first_seen'] > SPLIT_TIMESTAMP]\n",
    "\n",
    "print(f'Total #samples in the traintest_sandbox_metadata seen after <SPLIT_TIMESTAMP>: {len(sb_test_samples_in_full)}')\n",
    "print(f'Total #samples in the traintest_endpoint_metadata seen after <SPLIT_TIMESTAMP>: {len(ep_test_samples_in_full)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
